{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa947de",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import graph_tool.all as gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9555e10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph_world.beam.generator_config_sampler import ParamSamplerSpec\n",
    "from graph_world.substructure.beam_handler import SampleSubstructureDatasetDoFn, ConvertToTorchGeoDataParDo\n",
    "from graph_world.substructure.simulator import Substructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "238e7406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get graph data\n",
    "param_sampler_specs = [\n",
    "    ParamSamplerSpec(name=\"num_graphs\",\n",
    "                     min_val=1000,\n",
    "                     max_val=1000),\n",
    "    ParamSamplerSpec(name=\"num_vertices\",\n",
    "                     min_val=10,\n",
    "                     max_val=10),\n",
    "    ParamSamplerSpec(name=\"edge_prob\",\n",
    "                     min_val=0.6,\n",
    "                     max_val=0.6),\n",
    "    ParamSamplerSpec(name=\"train_prob\",\n",
    "                     min_val=0.6,\n",
    "                     max_val=0.6)\n",
    "]\n",
    "\n",
    "sampler_dofn = SampleSubstructureDatasetDoFn(\n",
    "    param_sampler_specs, Substructure.TAILED_TRIANGLE_GRAPH\n",
    ")\n",
    "\n",
    "sampler_out = next(sampler_dofn.process(sample_id=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0a82110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to torchgeo data\n",
    "convert_dofn = ConvertToTorchGeoDataParDo(output_path=\"/tmp\", batch_size=64)\n",
    "convert_out = next(convert_dofn.process(sampler_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b85da4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute graph metrics\n",
    "from graph_world.substructure.beam_handler import ComputeSubstructureGraphMetricsParDo\n",
    "metrics_dofn = ComputeSubstructureGraphMetricsParDo()\n",
    "metrics_out = next(metrics_dofn.process(convert_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bcf8abcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try GCN training with graph-world library fns\n",
    "import gin\n",
    "\n",
    "from graph_world.models.benchmarker import Benchmarker, BenchmarkGNNParDo\n",
    "from graph_world.models.wrappers import LinearGraphGCNWrapper\n",
    "\n",
    "gin.bind_parameter('LinearGraphGCNWrapper.num_features', 1)\n",
    "gin.bind_parameter('LinearGraphGCNWrapper.hidden_channels', 16)\n",
    "gin.bind_parameter('LinearGraphGCNWrapper.epochs', 100)\n",
    "gin.bind_parameter('LinearGraphGCNWrapper.lr', 0.0001)\n",
    "gin.bind_parameter('LinearGraphGCNWrapper.model_name', 'LinearGraphGCN')\n",
    "\n",
    "benchmarker_wrappers = [\n",
    "    LinearGraphGCNWrapper,\n",
    "]\n",
    "benchmarker_dofn = BenchmarkGNNParDo(benchmarker_wrappers)\n",
    "benchmarker_dofn.SetOutputPath('/tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19a30652",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmarker_out = next(benchmarker_dofn.process(metrics_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bb4dad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Copy GCN model from https://colab.sandbox.google.com/drive/1I8a0DfQ3fI7Njc62__mVXUlcAleUclnb?usp=sharing#scrollTo=HvhgQoO8Svw4\n",
    "import torch\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, num_node_features):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(num_node_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.lin = Linear(hidden_channels, 1)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index)\n",
    "\n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "model = GCN(16, 1)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d673f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy GCN training from https://colab.sandbox.google.com/drive/1I8a0DfQ3fI7Njc62__mVXUlcAleUclnb?usp=sharing#scrollTo=HvhgQoO8Svw4\n",
    "model = GCN(hidden_channels=16, num_node_features=1)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "train_loader = convert_out['torch_dataset']['train']\n",
    "test_loader = convert_out['torch_dataset']['test']\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    for data in train_loader:  # Iterate in batches over the training dataset.\n",
    "         out = model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.\n",
    "         loss = criterion(out, data.y)  # Compute the loss.\n",
    "         loss.backward()  # Derive gradients.\n",
    "         optimizer.step()  # Update parameters based on gradients.\n",
    "         optimizer.zero_grad()  # Clear gradients.\n",
    "\n",
    "def test(loader):\n",
    "     model.eval()\n",
    "\n",
    "     correct = 0\n",
    "     for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "         out = model(data.x, data.edge_index, data.batch)  \n",
    "         pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "         correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "     return correct / len(loader.dataset)  # Derive ratio of correct predictions.\n",
    "\n",
    "def test_mse(loader):\n",
    "     model.eval()\n",
    "\n",
    "     total_sse = 0.0\n",
    "     for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "         out = model(data.x, data.edge_index, data.batch)  \n",
    "         mse = float(criterion(out[:, 0], data.y))\n",
    "         total_sse += mse * data.batch.size().numel()\n",
    "     return total_sse / len(loader.dataset)\n",
    "\n",
    "\n",
    "for epoch in range(1, 171):\n",
    "    train()\n",
    "    train_acc = test(train_loader)\n",
    "    test_acc = test_mse(test_loader)\n",
    "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
